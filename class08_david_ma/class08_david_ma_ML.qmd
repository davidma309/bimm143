---
title: "Class 08: Machine Learning Mini Project"
author: "David Ma"
format: pdf
---

# Breast Cancer Project

Exploring data from the University of Wisconsin Cancer Center on breast biopsy data.

```{r}
wisc.data <- read.csv("WisconsinCancer.csv", row.names = 1)
head(wisc.data)
```


> Q1. How many observations are in this dataset?

```{r}
nrow(wisc.data)
```

There are `r nrow(wisc.data)` patients in this dataset.

> Q2. How many of the observations have a malignant diagnosis?

```{r}
table(wisc.data[, 1])
```

> Q3. How many variables/feature in the data are suffixed with _mean?

```{r}
length(grep("_mean$", names(wisc.data)))
```

Save the diagnosis for later use as a reference to compare how well we do with PCA etc.

```{r}
diagnosis <- as.factor(wisc.data[, 1])
diagnosis
```

Now, exclude the diagnosis column from the data

```{r}
wisc <- wisc.data[, -1]
```

> Q. How many "dimensions", "variables", and "columns" are there in this dataset?

```{r}
dim(wisc)
ncol(wisc)
```

# Principal Component Analysis (PCA)

To perform PCA in R we can use the `prcomp()` function. It takes a numerical dataset input and optional `scale=FALSE/TRUE` argument.

Generally, `scale=TRUE` should always be set but let's make sure by checking if the mean and standard deviation are different across these 30 columns.

```{r}
round( colMeans(wisc))
```

```{r}
pca <- prcomp(wisc, scale = TRUE)
summary(pca)
```
> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

44.27% is captured by PC1.

>Q5. How many principal components are required to describe at least 70% of the original variance in the data?

Three principal components are required.

> Q6. How many principal components are required to describe at least 90% of the original variance in the data?

Seven principal components are required
```{r}
attributes(pca)
```
> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

```{r}
biplot(pca)
```

This plot stands out because it is so hard to actually read anything! 

```{r}
plot(pca$x[,1], pca$x[,2], col = diagnosis)
```

```{r}
library(ggplot2)

x <- as.data.frame(pca$x)

ggplot(x) +
  aes(PC1, PC2, col = diagnosis) +
  geom_point()
```

> Q8. Generate a simplar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
y <- as.data.frame(pca$x)

ggplot(y) +
  aes(PC1, PC3, col = diagnosis) +
  geom_point()
```

I notice that these two subgroups are a little harder to tell apart because component 3 explains less variance in the original data than component 2.

> Q9.  For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean? 

```{r}
pca$rotation["concave.points_mean", 1]
```

> Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

5 principal components

# Hierachical Clustering

```{r}
# Scale the wisc data using the "scale()" function
data.scaled <- scale(wisc)
```

```{r}
# Calculating Euclidean distances between all pairs of observations
data.dist <- dist(data.scaled)
```

```{r}
# Creating hierarchical clustering using complete linkage
wisc.hclust <- hclust(data.dist, method = "complete")
```

> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(h = 19, col = "red", lty = 2)
```

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)
table(wisc.hclust.clusters, diagnosis)
```

> Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k = 3)
table(wisc.hclust.clusters, diagnosis)
```

> Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning

I like ward.D2 a bit because it minimizes variance within clusters while working from the bottom up when creating the tree.

```{r}
attributes(pca)
```

# Combining PCA results with clustering

We can use our new PCA variables (i.e. the scores along the PCs contained in `pca$x`) as input for other methods such as clustering.

```{r}
# Hclust needs a distance matrix as input
d <- dist(pca$x[,1:3])

hc <- hclust(d, method = "ward.D2")
plot(hc)
```

To get our cluster membership vector we can use the `cutree()` function and specify a height (`h`) or number of groups (`k`).

```{r}
grps <- cutree(hc, h = 80)
table(grps)
```

I want to find out how many diagnosis "M" and "B" are in each group.

```{r}
table(diagnosis, grps)
```

We can also plot our results using our clustering vector `grps`

```{r}
plot(pca$x[,1], pca$x[,2], col = grps)
```

```{r}
ggplot(x) +
  aes(PC1, PC2) +
  geom_point(col = grps)
```

> Q15. What is the specificity and sensitivity of our current results?

```{r}
# Specificity: True negative / (True negative + False negative)
333 / (333 + 33)

# Sensitivity: True positive / (True positive + False negative)
179 / (179 + 33)
```

